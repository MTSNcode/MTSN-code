{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated flags [('structural_head_config', u'16,8,8'), ('temporal_layer_config', u'128'), ('val_freq', 1), ('dataset', u'UCI'), ('epochs', 30), ('walk_len', 20), ('weight_decay', 0.0005), ('neg_sample_size', 10), ('temporal_drop', 0.5), ('GPU_ID', 0), ('use_residual', u'False'), ('structural_layer_config', u'128'), ('window', -1), ('base_model', u'DySAT'), ('log_dir', 'log'), ('model_dir', 'model'), ('spatial_drop', 0.1), ('position_ffn', u'True'), ('optimizer', 'adam'), ('time_steps', 9), ('learning_rate', 0.001), ('csv_dir', 'csv'), ('batch_size', 512), ('max_gradient_norm', 1.0), ('featureless', u'True'), ('f', '/root/.local/share/jupyter/runtime/kernel-b891e3c7-32b8-4ddf-9205-a68fffd6eda0.json'), ('test_freq', 1), ('temporal_head_config', u'16'), ('seed', 7), ('save_dir', 'output'), ('model', u'default'), ('neg_weight', 1.0)]\n",
      "Loaded 13 graphs \n",
      "Loaded context pairs from pkl file directly\n",
      "Loaded eval data\n",
      "# train: 73, # val: 71, # test: 214\n",
      "Loaded 13*8 motifs \n",
      "Initializing session\n",
      "# train nodes 1750\n",
      "# training batches per epoch 4\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "feature transform time: 0.0006s\n",
      "each shift time:0.0155s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0161s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0154s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0154s\n",
      "each SGC time:0.0146s\n",
      "each shift time:0.0155s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0155s\n",
      "each SGC time:0.0148s\n",
      "each shift time:0.0161s\n",
      "each SGC time:0.1848s\n",
      "each shift time:0.0157s\n",
      "each SGC time:0.0142s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0145s\n",
      "each shift time:0.0155s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0154s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0159s\n",
      "each SGC time:0.0142s\n",
      "each shift time:0.0154s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0162s\n",
      "each SGC time:0.0142s\n",
      "each shift time:0.0160s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0154s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0159s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0138s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0146s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0146s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0140s\n",
      "each shift time:0.0161s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0141s\n",
      "each shift time:0.0153s\n",
      "each SGC time:0.0146s\n",
      "loss time:0.3910s\n",
      "Variable:0\n",
      "motifs_weight:0\n",
      "sgclayer_1_vars/SGC_weight:0\n",
      "weight_fea:0\n",
      "weight_fea_final:0\n",
      "weight_emb:0\n",
      "Epoch: 0001\n",
      "Time for epoch  4.38727402687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val AUC 0.642134497124\n",
      "Epoch 0, Test AUC 0.622521617609\n",
      "Mean Loss at epoch 0 : 11.0866279602\n",
      "Epoch: 0002\n",
      "Time for epoch  3.83633303642\n",
      "Epoch 1, Val AUC 0.738543939695\n",
      "Epoch 1, Test AUC 0.740763385449\n",
      "Mean Loss at epoch 1 : 11.0848798752\n",
      "Epoch: 0003\n",
      "Time for epoch  3.46129894257\n",
      "Epoch 2, Val AUC 0.778416980758\n",
      "Epoch 2, Test AUC 0.80321425452\n",
      "Mean Loss at epoch 2 : 11.0817763805\n",
      "Epoch: 0004\n",
      "Time for epoch  3.77499890327\n",
      "Epoch 3, Val AUC 0.777425114065\n",
      "Epoch 3, Test AUC 0.819351035025\n",
      "Mean Loss at epoch 3 : 11.0738012791\n",
      "Epoch: 0005\n",
      "Time for epoch  3.89102315903\n",
      "Epoch 4, Val AUC 0.770085300536\n",
      "Epoch 4, Test AUC 0.817276618045\n",
      "Mean Loss at epoch 4 : 11.0703141689\n",
      "Epoch: 0006\n",
      "Time for epoch  3.58313822746\n",
      "Epoch 5, Val AUC 0.765522713747\n",
      "Epoch 5, Test AUC 0.810463795965\n",
      "Mean Loss at epoch 5 : 11.0517852306\n",
      "Epoch: 0007\n",
      "Time for epoch  3.83920288086\n",
      "Epoch 6, Val AUC 0.769291807181\n",
      "Epoch 6, Test AUC 0.803956677439\n",
      "Mean Loss at epoch 6 : 11.0326588154\n",
      "Epoch: 0008\n",
      "Time for epoch  3.73323917389\n",
      "Epoch 7, Val AUC 0.767704820472\n",
      "Epoch 7, Test AUC 0.801663900777\n",
      "Mean Loss at epoch 7 : 11.0484552383\n",
      "Epoch: 0009\n",
      "Time for epoch  3.53424978256\n",
      "Epoch 8, Val AUC 0.772465780599\n",
      "Epoch 8, Test AUC 0.803716481789\n",
      "Mean Loss at epoch 8 : 11.0985996723\n",
      "Epoch: 0010\n",
      "Time for epoch  3.85463285446\n",
      "Epoch 9, Val AUC 0.775441380678\n",
      "Epoch 9, Test AUC 0.807122892829\n",
      "Mean Loss at epoch 9 : 11.0818986893\n",
      "Epoch: 0011\n",
      "Time for epoch  3.82228112221\n",
      "Epoch 10, Val AUC 0.779805594128\n",
      "Epoch 10, Test AUC 0.811664774216\n",
      "Mean Loss at epoch 10 : 11.0482127666\n",
      "Epoch: 0012\n",
      "Time for epoch  3.46992206573\n",
      "Epoch 11, Val AUC 0.780797460821\n",
      "Epoch 11, Test AUC 0.812734736658\n",
      "Mean Loss at epoch 11 : 11.0568985939\n",
      "Epoch: 0013\n",
      "Time for epoch  4.1020026207\n",
      "Epoch 12, Val AUC 0.779210474112\n",
      "Epoch 12, Test AUC 0.812188837453\n",
      "Mean Loss at epoch 12 : 11.0335297585\n",
      "Epoch: 0014\n",
      "Time for epoch  3.86031079292\n",
      "Epoch 13, Val AUC 0.783177940885\n",
      "Epoch 13, Test AUC 0.81247270504\n",
      "Mean Loss at epoch 13 : 11.0502612591\n",
      "Epoch: 0015\n",
      "Time for epoch  3.69347214699\n",
      "Epoch 14, Val AUC 0.779210474112\n",
      "Epoch 14, Test AUC 0.809219145777\n",
      "Mean Loss at epoch 14 : 11.012665987\n",
      "Epoch: 0016\n",
      "Time for epoch  3.94722080231\n",
      "Epoch 15, Val AUC 0.77087879389\n",
      "Epoch 15, Test AUC 0.803105074679\n",
      "Mean Loss at epoch 15 : 11.0328803062\n",
      "Epoch: 0017\n",
      "Time for epoch  3.90887713432\n",
      "Epoch 16, Val AUC 0.765324340409\n",
      "Epoch 16, Test AUC 0.79834483361\n",
      "Mean Loss at epoch 16 : 11.0399281979\n",
      "Epoch: 0018\n",
      "Time for epoch  3.94631814957\n",
      "Epoch 17, Val AUC 0.75362031343\n",
      "Epoch 17, Test AUC 0.789763298105\n",
      "Mean Loss at epoch 17 : 11.0435402393\n",
      "Epoch: 0019\n",
      "Time for epoch  3.79266881943\n",
      "Epoch 18, Val AUC 0.746478873239\n",
      "Epoch 18, Test AUC 0.782688444406\n",
      "Mean Loss at epoch 18 : 11.0064661503\n",
      "Epoch: 0020\n",
      "Time for epoch  3.99473524094\n",
      "Epoch 19, Val AUC 0.742709779806\n",
      "Epoch 19, Test AUC 0.781705825836\n",
      "Mean Loss at epoch 19 : 11.0076601505\n",
      "Epoch: 0021\n",
      "Time for epoch  4.10518050194\n",
      "Epoch 20, Val AUC 0.741916286451\n",
      "Epoch 20, Test AUC 0.780876059044\n",
      "Mean Loss at epoch 20 : 11.0081365108\n",
      "Epoch: 0022\n",
      "Time for epoch  3.62192702293\n",
      "Epoch 21, Val AUC 0.740527673081\n",
      "Epoch 21, Test AUC 0.780046292253\n",
      "Mean Loss at epoch 21 : 11.001254797\n",
      "Epoch: 0023\n",
      "Time for epoch  4.02952980995\n",
      "Epoch 22, Val AUC 0.744098393176\n",
      "Epoch 22, Test AUC 0.783627391039\n",
      "Mean Loss at epoch 22 : 11.012193203\n",
      "Epoch: 0024\n",
      "Time for epoch  4.0369682312\n",
      "Epoch 23, Val AUC 0.749851219996\n",
      "Epoch 23, Test AUC 0.787099309983\n",
      "Mean Loss at epoch 23 : 11.028724432\n",
      "Epoch: 0025\n",
      "Time for epoch  3.80251383781\n",
      "Epoch 24, Val AUC 0.749454473319\n",
      "Epoch 24, Test AUC 0.786335051096\n",
      "Mean Loss at epoch 24 : 10.9897525311\n",
      "Epoch: 0026\n",
      "Time for epoch  3.99463772774\n",
      "Epoch 25, Val AUC 0.751239833366\n",
      "Epoch 25, Test AUC 0.786706262556\n",
      "Mean Loss at epoch 25 : 11.0001952648\n",
      "Epoch: 0027\n",
      "Time for epoch  4.05896401405\n",
      "Epoch 26, Val AUC 0.7550089268\n",
      "Epoch 26, Test AUC 0.787645209189\n",
      "Mean Loss at epoch 26 : 10.9848711491\n",
      "Epoch: 0028\n",
      "Time for epoch  3.92057085037\n",
      "Epoch 27, Val AUC 0.752430073398\n",
      "Epoch 27, Test AUC 0.786007511573\n",
      "Mean Loss at epoch 27 : 11.015175581\n",
      "Epoch: 0029\n",
      "Time for epoch  4.0334789753\n",
      "Epoch 28, Val AUC 0.752628446737\n",
      "Epoch 28, Test AUC 0.785942003668\n",
      "Mean Loss at epoch 28 : 10.9651026726\n",
      "Epoch: 0030\n",
      "Time for epoch  4.15424919128\n",
      "Epoch 29, Val AUC 0.751636580044\n",
      "Epoch 29, Test AUC 0.784937549131\n",
      "Mean Loss at epoch 29 : 10.9669311047\n",
      "Best epoch  13\n",
      "Best epoch val results defaultdict(<function <lambda> at 0x7f8a404a12a8>, {'HAD': [0.783177940884745, 0.676056338028169, 0.8164544967519064], 'SIGMOID': [0.7754413806784368, 0.676056338028169, 0.8100214314128309]})\n",
      "\n",
      "Best epoch test results defaultdict(<function <lambda> at 0x7f8a404a1500>, {'HAD': [0.8124727050397414, 0.7196261682242989, 0.8327431580787301], 'SIGMOID': [0.8057690628002445, 0.7196261682242989, 0.8273283240842976]})\n",
      "\n",
      "default results (test) [0.8124727050397414, 0.7196261682242989, 0.8327431580787301]\n",
      "default results (test) [0.8057690628002445, 0.7196261682242989, 0.8273283240842976]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "import scipy\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "from eval.link_prediction import evaluate_classifier, write_to_csv\n",
    "from flags import *\n",
    "from models.DySAT.models import DySAT\n",
    "from utils.minibatch import *\n",
    "from utils.preprocess import *\n",
    "from utils.utilities import *\n",
    "\n",
    "# test\n",
    "import pdb\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def convert_to_sp_tensor(X):\n",
    "    if type(X) == 'csr_matrix':\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        sptensor = tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    else:\n",
    "        sptensor = tf.SparseTensor(X[0], X[1], X[2])\n",
    "    return sptensor\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "# FLAGS.dataset = 'Enron_new'\n",
    "# FLAGS.dataset = 'ml-10m_new'\n",
    "FLAGS.dataset = 'UCI'\n",
    "# FLAGS.dataset = 'yelp'\n",
    "# FLAGS.dataset = 'DBLP'\n",
    "# FLAGS.dataset = 'Epinions'\n",
    "# FLAGS.dataset = 'alibaba'\n",
    "\n",
    "\n",
    "use_motifs = False\n",
    "use_motifs = True\n",
    "\n",
    "motifs_number = 8\n",
    "\n",
    "# Assumes a saved base model as input and model name to get the right directory.\n",
    "output_dir = \"./logs/{}_{}/\".format(FLAGS.base_model, FLAGS.model)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "config_file = output_dir + \"flags_{}.json\".format(FLAGS.dataset)\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "    for name, value in config.items():\n",
    "        if name in FLAGS.__flags:\n",
    "            FLAGS.__flags[name].value = value\n",
    "\n",
    "print(\"Updated flags\", FLAGS.flag_values_dict().items())\n",
    "\n",
    "# Set paths of sub-directories.\n",
    "LOG_DIR = output_dir + FLAGS.log_dir\n",
    "SAVE_DIR = output_dir + FLAGS.save_dir\n",
    "CSV_DIR = output_dir + FLAGS.csv_dir\n",
    "MODEL_DIR = output_dir + FLAGS.model_dir\n",
    "\n",
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR):\n",
    "    os.mkdir(SAVE_DIR)\n",
    "\n",
    "if not os.path.isdir(CSV_DIR):\n",
    "    os.mkdir(CSV_DIR)\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(FLAGS.GPU_ID)\n",
    "\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "today = datetime.today()\n",
    "\n",
    "# Setup logging\n",
    "log_file = LOG_DIR + '/%s_%s_%s_%s_%s.log' % (FLAGS.dataset.split(\"/\")[0], str(today.year),\n",
    "                                              str(today.month), str(today.day), str(FLAGS.time_steps))\n",
    "\n",
    "log_level = logging.INFO\n",
    "logging.basicConfig(filename=log_file, level=log_level, format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "logging.info(FLAGS.flag_values_dict().items())\n",
    "\n",
    "# Create file name for result log csv from certain flag parameters.\n",
    "output_file = CSV_DIR + '/%s_%s_%s_%s.csv' % (FLAGS.dataset.split(\"/\")[0], str(today.year),\n",
    "                                              str(today.month), str(today.day))\n",
    "\n",
    "# model_dir is not used in this code for saving.\n",
    "\n",
    "# utils folder: utils.py, random_walk.py, minibatch.py\n",
    "# models folder: layers.py, models.py\n",
    "# main folder: train.py\n",
    "# eval folder: link_prediction.py\n",
    "\n",
    "\"\"\"\n",
    "#1: Train logging format: Create a new log directory for each run (if log_dir is provided as input). \n",
    "Inside it,  a file named <>.log will be created for each time step. The default name of the directory is \"log\" and the \n",
    "contents of the <>.log will get appended per day => one log file per day.\n",
    "#2: Model save format: The model is saved inside model_dir. \n",
    "#3: Output save format: Create a new output directory for each run (if save_dir name is provided) with embeddings at \n",
    "each \n",
    "time step. By default, a directory named \"output\" is created.\n",
    "#4: Result logging format: A csv file will be created at csv_dir and the contents of the file will get over-written \n",
    "as per each day => new log file for each day.\n",
    "\"\"\"\n",
    "\n",
    "# Load graphs and features.\n",
    "\n",
    "num_time_steps = FLAGS.time_steps\n",
    "\n",
    "graphs, adjs = load_graphs(FLAGS.dataset)\n",
    "if FLAGS.featureless:\n",
    "    feats = [scipy.sparse.identity(adjs[num_time_steps - 1].shape[0]).tocsr()[range(0, x.shape[0]), :] for x in adjs if\n",
    "             x.shape[0] <= adjs[num_time_steps - 1].shape[0]]\n",
    "else:\n",
    "    feats = load_feats(FLAGS.dataset)\n",
    "\n",
    "num_features = feats[0].shape[1]\n",
    "assert num_time_steps < len(adjs) + 1  # So that, (t+1) can be predicted.\n",
    "\n",
    "adj_train = []\n",
    "motifs_train = []\n",
    "motifs_train_1 = []\n",
    "feats_train = []\n",
    "num_features_nonzero = []\n",
    "loaded_pairs = False\n",
    "\n",
    "# Load training context pairs (or compute them if necessary)\n",
    "context_pairs_train = get_context_pairs(graphs, num_time_steps)\n",
    "\n",
    "# Load evaluation data.\n",
    "train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "    get_evaluation_data(adjs, num_time_steps, FLAGS.dataset)\n",
    "\n",
    "# Create the adj_train so that it includes nodes from (t+1) but only edges from t: this is for the purpose of\n",
    "# inductive testing.\n",
    "new_G = nx.MultiGraph()\n",
    "new_G.add_nodes_from(graphs[num_time_steps - 1].nodes(data=True))\n",
    "\n",
    "for e in graphs[num_time_steps - 2].edges():\n",
    "    new_G.add_edge(e[0], e[1])\n",
    "\n",
    "graphs[num_time_steps - 1] = new_G\n",
    "adjs[num_time_steps - 1] = nx.adjacency_matrix(new_G)\n",
    "\n",
    "print(\"# train: {}, # val: {}, # test: {}\".format(len(train_edges), len(val_edges), len(test_edges)))\n",
    "logging.info(\"# train: {}, # val: {}, # test: {}\".format(len(train_edges), len(val_edges), len(test_edges)))\n",
    "\n",
    "# Normalize and convert adj. to sparse tuple format (to provide as input via SparseTensor)\n",
    "adj_train = map(lambda adj: normalize_graph_gcn(adj), adjs)\n",
    "\n",
    "# TODO load time_step*36 motif matrix and normalize motif matrix\n",
    "motifs = load_motifs(FLAGS.dataset)\n",
    "\n",
    "for time_index in range(len(motifs)):\n",
    "    temp_motifs = []\n",
    "    temp_motifs_1 = []\n",
    "    # for iter_index in range(len(motifs[time_index])):\n",
    "    for iter_index in range(motifs_number):\n",
    "        ttt = motifs[time_index][iter_index]\n",
    "        # temp_motifs_1.append(convert_to_sp_tensor(normalize_graph_gcn(motifs[time_index][iter_index])))\n",
    "        temp_motifs.append(normalize_motif_gcn(motifs[time_index][iter_index]))\n",
    "    # motifs_train_1.append(temp_motifs_1)\n",
    "    motifs_train.append(temp_motifs)\n",
    "# motifs_train = np.array(motifs_train)\n",
    "# testdata = tf.data.Dataset.from_sparse_tensor_slices(motifs_train_1)\n",
    "\n",
    "# then feed motif matrix to model\n",
    "# use weight matrix fuse motif matrix and adj\n",
    "\n",
    "if FLAGS.featureless:  # Use 1-hot matrix in case of featureless.\n",
    "    feats = [scipy.sparse.identity(adjs[num_time_steps - 1].shape[0]).tocsr()[range(0, x.shape[0]), :] for x in feats if\n",
    "             x.shape[0] <= feats[num_time_steps - 1].shape[0]]\n",
    "num_features = feats[0].shape[1]\n",
    "\n",
    "feats_train = map(lambda feat: preprocess_features(feat)[1], feats)\n",
    "num_features_nonzero = [x[1].shape[0] for x in feats_train]\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "def construct_placeholders(num_time_steps, adjs):\n",
    "    temp = int(FLAGS.structural_layer_config)\n",
    "    min_t = 0\n",
    "    if FLAGS.window > 0:\n",
    "        min_t = max(num_time_steps - FLAGS.window - 1, 0)\n",
    "    placeholders = {\n",
    "        'node_1': [tf.placeholder(tf.int32, shape=(None,), name=\"node_1\") for _ in range(min_t, num_time_steps)],\n",
    "        # [None,1] for each time step.\n",
    "        'node_2': [tf.placeholder(tf.int32, shape=(None,), name=\"node_2\") for _ in range(min_t, num_time_steps)],\n",
    "        # [None,1] for each time step.\n",
    "        'batch_nodes': tf.placeholder(tf.int32, shape=(None,), name=\"batch_nodes\"),  # [None,1]\n",
    "        # 'features': [tf.sparse_placeholder(tf.float32, shape=(None, num_features), name=\"feats\") for _ in\n",
    "        #              range(min_t, num_time_steps)],\n",
    "        # 'adjs': [tf.sparse_placeholder(tf.float32, shape=(None, None), name=\"adjs\") for i in\n",
    "        #          range(min_t, num_time_steps)],\n",
    "        'features': [tf.sparse_placeholder(tf.float32, shape=(adjs[_].shape[0], num_features), name=\"feats\") for _ in\n",
    "                     range(min_t, num_time_steps)],\n",
    "        'adjs': [tf.sparse_placeholder(tf.float32, shape=(adjs[j].shape[0], adjs[j].shape[0]), name=\"adjs\") for j in\n",
    "                 range(min_t, num_time_steps)],\n",
    "        'spatial_drop': tf.placeholder(dtype=tf.float32, shape=(), name='spatial_drop'),\n",
    "        'temporal_drop': tf.placeholder(dtype=tf.float32, shape=(), name='temporal_drop'),\n",
    "        # SGC weight\n",
    "        # 'SGC_weight': tf.Variable(tf.random_uniform([int(FLAGS.structural_layer_config), int(FLAGS.structural_layer_config)]), trainable=True),\n",
    "        # 'SGC_weight': tf.Variable(tf.random.randn([int(FLAGS.structural_layer_config), int(FLAGS.structural_layer_config)]), trainable=True),\n",
    "        # 'motifs_weight': tf.Variable(tf.random_uniform([1,1,len(motifs[0])]), trainable=True),\n",
    "        # time*36*node_num*node_num\n",
    "        # 'motifs{}'.format(min_t): [tf.sparse_placeholder(tf.float32, shape=(None, None), name=\"motifs{}\".format(min_t)) for i in range(min_t, num_time_steps)]\n",
    "    }\n",
    "    for t in range(min_t, num_time_steps):\n",
    "        placeholders['motifs{}'.format(t)] = [tf.sparse_placeholder(tf.float32, shape=(adjs[t].shape[0], adjs[t].shape[0]), name=\"motifs{}\".format(t)) for i in range(len(motifs_train[0]))]\n",
    "    return placeholders\n",
    "\n",
    "\n",
    "SGC_weight = tf.Variable(tf.zeros((int(FLAGS.structural_layer_config), int(FLAGS.structural_layer_config))), trainable=True)\n",
    "\n",
    "weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "motifs_weight = tf.get_variable(name='motifs_weight',shape=[1,1,motifs_number,],initializer=weight_initer)\n",
    "# motifs_weight = tf.Variable(tf.zeros((1,1,motifs_number,)), trainable=True, name='motifs_weight')\n",
    "# motifs_weight = tf.Variable(tf.ones((1,1,motifs_number,)), trainable=False, name='motifs_weight')\n",
    "\n",
    "min_t = 0\n",
    "if FLAGS.window > 0:\n",
    "    min_t = max(num_time_steps - FLAGS.window - 1, 0)\n",
    "\n",
    "all_time = time.time()\n",
    "print(\"Initializing session\")\n",
    "# Initialize session\n",
    "config = tf.ConfigProto(device_count={\"CPU\":8},inter_op_parallelism_threads=0, intra_op_parallelism_threads=0)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "placeholders = construct_placeholders(num_time_steps,adjs)\n",
    "\n",
    "minibatchIterator = NodeMinibatchIterator(graphs, feats_train, adj_train, motifs_train,\n",
    "                                          placeholders, num_time_steps, batch_size=FLAGS.batch_size,\n",
    "                                          context_pairs=context_pairs_train)\n",
    "print(\"# training batches per epoch\", minibatchIterator.num_training_batches())\n",
    "\n",
    "\n",
    "model = DySAT(placeholders, SGC_weight, motifs_weight, use_motifs, num_features, num_features_nonzero, minibatchIterator.degs, min_t, num_time_steps)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(v.name)\n",
    "\n",
    "# print(sess.run(tf.report_uninitialized_variables()))\n",
    "\n",
    "# Result accumulator variables.\n",
    "epochs_test_result = defaultdict(lambda: [])\n",
    "epochs_val_result = defaultdict(lambda: [])\n",
    "epochs_embeds = []\n",
    "epochs_attn_wts_all = []\n",
    "max_epoch_auc_val = -1\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    minibatchIterator.shuffle()\n",
    "    epoch_loss = 0.0\n",
    "    it = 0\n",
    "    print('Epoch: %04d' % (epoch + 1))\n",
    "    epoch_time = 0.0\n",
    "    while not minibatchIterator.end():\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatchIterator.next_minibatch_feed_dict()\n",
    "        feed_dict.update({placeholders['spatial_drop']: FLAGS.spatial_drop})\n",
    "        feed_dict.update({placeholders['temporal_drop']: FLAGS.temporal_drop})\n",
    "        t = time.time()\n",
    "        # Training step\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, train_cost, graph_cost, reg_cost = sess.run([model.opt_op, model.loss, model.graph_loss, model.reg_loss],\n",
    "                                                       feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n",
    "        \n",
    "         # Create the Timeline object, and write it to a json\n",
    "        tl = timeline.Timeline(run_metadata.step_stats)\n",
    "        ctf = tl.generate_chrome_trace_format()\n",
    "        with open('timeline.json', 'w') as fline:\n",
    "            fline.write(ctf)\n",
    "\n",
    "        # print(sess.run(SGC_weight,feed_dict))\n",
    "        # print(sess.run(motifs_weight, feed_dict))\n",
    "\n",
    "        epoch_time += time.time() - t\n",
    "        # Print results\n",
    "        logging.info(\"Mini batch Iter: {} train_loss= {:.5f}\".format(it, train_cost))\n",
    "        logging.info(\"Mini batch Iter: {} graph_loss= {:.5f}\".format(it, graph_cost))\n",
    "        logging.info(\"Mini batch Iter: {} reg_loss= {:.5f}\".format(it, reg_cost))\n",
    "        logging.info(\"Time for Mini batch : {}\".format(time.time() - t))\n",
    "\n",
    "        epoch_loss += train_cost\n",
    "        it += 1\n",
    "\n",
    "    print(\"Time for epoch \", epoch_time)\n",
    "    logging.info(\"Time for epoch : {}\".format(epoch_time))\n",
    "    if (epoch + 1) % FLAGS.test_freq == 0:\n",
    "        minibatchIterator.test_reset()\n",
    "        emb = []\n",
    "        feed_dict.update({placeholders['spatial_drop']: 0.0})\n",
    "        feed_dict.update({placeholders['temporal_drop']: 0.0})\n",
    "        if FLAGS.window < 0:\n",
    "            assert FLAGS.time_steps == model.final_output_embeddings.get_shape()[1]\n",
    "\n",
    "        if use_motifs:        \n",
    "#             adj_out = sess.run(model.adj_out, feed_dict=feed_dict)\n",
    "\n",
    "#             motif_out = sess.run(model.motif_out, feed_dict=feed_dict)\n",
    "            \n",
    "            motif_weight_temp = sess.run(model.motif_weight_out, feed_dict=feed_dict)\n",
    "\n",
    "        emb = sess.run(model.final_output_embeddings, feed_dict=feed_dict)[:,\n",
    "              model.final_output_embeddings.get_shape()[1] - 2, :]\n",
    "        emb = np.array(emb)\n",
    "        # pdb.set_trace()\n",
    "        # Use external classifier to get validation and test results.\n",
    "        val_results, test_results, _, _ = evaluate_classifier(train_edges,\n",
    "                                                              train_edges_false, val_edges, val_edges_false, test_edges,\n",
    "                                                              test_edges_false, emb, emb)\n",
    "\n",
    "        epoch_auc_val = val_results[\"HAD\"][0]\n",
    "        epoch_auc_test = test_results[\"HAD\"][0]\n",
    "\n",
    "        print(\"Epoch {}, Val AUC {}\".format(epoch, epoch_auc_val))\n",
    "        print(\"Epoch {}, Test AUC {}\".format(epoch, epoch_auc_test))\n",
    "        logging.info(\"Val results at epoch {}: Measure ({}) AUC: {}\".format(epoch, \"HAD\", epoch_auc_val))\n",
    "        logging.info(\"Test results at epoch {}: Measure ({}) AUC: {}\".format(epoch, \"HAD\", epoch_auc_test))\n",
    "\n",
    "        epochs_test_result[\"HAD\"].append(epoch_auc_test)\n",
    "        epochs_val_result[\"HAD\"].append(epoch_auc_val)\n",
    "        # epochs_embeds.append(emb)\n",
    "\n",
    "        # do not save all embs\n",
    "        # only save best embs\n",
    "        if epoch_auc_val > max_epoch_auc_val:\n",
    "            max_epoch_auc_val = epoch_auc_val\n",
    "            epochs_embeds = emb # save best emb\n",
    "            if use_motifs:\n",
    "                save_motif_weight = motif_weight_temp\n",
    "        if epoch_auc_val == 1:\n",
    "            max_epoch_auc_val = epoch_auc_val\n",
    "            epochs_embeds = emb # save best emb\n",
    "        \n",
    "    epoch_loss /= it\n",
    "    print(\"Mean Loss at epoch {} : {}\".format(epoch, epoch_loss))\n",
    "\n",
    "# Choose best model by validation set performance.\n",
    "best_epoch = epochs_val_result[\"HAD\"].index(max(epochs_val_result[\"HAD\"]))\n",
    "\n",
    "print(\"Best epoch \", best_epoch)\n",
    "logging.info(\"Best epoch {}\".format(best_epoch))\n",
    "\n",
    "# val_results, test_results, _, _ = evaluate_classifier(train_edges, train_edges_false, val_edges, val_edges_false,\n",
    "#                                                       test_edges, test_edges_false, epochs_embeds[best_epoch],\n",
    "#                                                       epochs_embeds[best_epoch])\n",
    "\n",
    "val_results, test_results, _, _ = evaluate_classifier(train_edges, train_edges_false, val_edges, val_edges_false,\n",
    "                                                      test_edges, test_edges_false, epochs_embeds,\n",
    "                                                      epochs_embeds)\n",
    "\n",
    "print(\"Best epoch val results {}\\n\".format(val_results))\n",
    "print(\"Best epoch test results {}\\n\".format(test_results))\n",
    "\n",
    "logging.info(\"Best epoch val results {}\\n\".format(val_results))\n",
    "logging.info(\"Best epoch test results {}\\n\".format(test_results))\n",
    "\n",
    "all_time = time.time()-all_time\n",
    "\n",
    "# write_to_csv(val_results, output_file, FLAGS.model, FLAGS.dataset, num_time_steps, all_time, mod='val')\n",
    "with open(output_file, 'a+') as f:\n",
    "    f.write('motif_number: {}\\tuse motif: {}\\n'.format(motifs_number, use_motifs))\n",
    "    if use_motifs:\n",
    "        f.write('motif_weight: {}\\n'.format(save_motif_weight))\n",
    "\n",
    "write_to_csv(test_results, output_file, FLAGS.model, FLAGS.dataset, num_time_steps, all_time, mod='test')\n",
    "\n",
    "# Save final embeddings in the save directory.\n",
    "emb = epochs_embeds\n",
    "# np.savez(SAVE_DIR + '/{}_embs_{}_{}.npz'.format(FLAGS.model, FLAGS.dataset, FLAGS.time_steps - 2), data=emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           125G        3.2G         11G        6.7M        111G        121G\r\n",
      "Swap:          127G        9.3M        127G\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(GPU_ID=0, base_model='DySAT', batch_size=512, dataset='UCI', epochs=30, featureless='False', learning_rate=0.001, max_gradient_norm=1.0, max_time=12, min_time=2, model='default', neg_sample_size=10, neg_weight=1.0, position_ffn='True', run_parallel='False', spatial_drop=0.1, structural_head_config='16,8,8', structural_layer_config='512', temporal_drop=0.5, temporal_head_config='16', temporal_layer_config='128', test_freq=1, use_residual='False', val_freq=1, walk_len=20, weight_decay=0.0005, window=-1)\n",
      "('Args parallel param: ', 'False')\n",
      "Running time steps 2 to 12 sequentially on GPU 0\n",
      "('Call ', 'python train_old.py --time_steps 2 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 3 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 4 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 5 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 6 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 7 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 8 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 9 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 10 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 11 --base_model DySAT --model default --dataset UCI')\n",
      "('Call ', 'python train_old.py --time_steps 12 --base_model DySAT --model default --dataset UCI')\n"
     ]
    }
   ],
   "source": [
    "from subprocess import call\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from subprocess import Popen\n",
    "import os, json, time\n",
    "\n",
    "\"\"\"NOTE: This script includes nearly all tf flag parameters as input arguments, which feed as input \n",
    "through a generated config file.\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Run script parameters')\n",
    "\n",
    "# Script specific parameters -> min and max time steps for executing different train files.\n",
    "# Time step range [min_time, max_time to train different models (both included).\n",
    "# Min time step is always 2 since we require at least one snapshot each for train and test.\n",
    "\n",
    "parser.add_argument('--min_time', type=int, nargs='?', default=2, help='min_time step')\n",
    "\n",
    "parser.add_argument('--max_time', type=int, nargs='?', default=12, help='max_time step')\n",
    "\n",
    "# NOTE: Ensure that the execution is split into different ranges so that GPU memory errors are avoided.\n",
    "# IncSAT must be executed sequentially\n",
    "\n",
    "parser.add_argument('--run_parallel', type=str, nargs='?', default='False',\n",
    "                    help='By default, sequential execution of different time steps (Note: IncSAT must be sequential)')\n",
    "\n",
    "# Necessary parameters for log creation.\n",
    "parser.add_argument('--base_model', type=str, nargs='?', default='DySAT',\n",
    "                    help='Base model (DySAT/IncSAT)')\n",
    "\n",
    "# Additional model string to save different parameter variations.\n",
    "parser.add_argument('--model', type=str, nargs='?', default='default',\n",
    "                    help='Additional model string')\n",
    "\n",
    "# Experimental settings.\n",
    "parser.add_argument('--dataset', type=str, nargs='?', default='UCI',\n",
    "                    help='dataset name') # 'Enron_new' # 'ml-10m_new' # 'UCI' # 'yelp' # 'Epinions' # 'alibaba'\n",
    "\n",
    "parser.add_argument('--GPU_ID', type=int, nargs='?', default=0,\n",
    "                    help='GPU_ID (0/1 etc.)')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, nargs='?', default=30,\n",
    "                    help='# epochs')\n",
    "\n",
    "parser.add_argument('--val_freq', type=int, nargs='?', default=1,\n",
    "                    help='Validation frequency (in epochs)')\n",
    "\n",
    "parser.add_argument('--test_freq', type=int, nargs='?', default=1,\n",
    "                    help='Testing frequency (in epochs)')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, nargs='?', default=512,\n",
    "                    help='Batch size (# nodes)')\n",
    "\n",
    "# 1-hot encoding is input as a sparse matrix - hence no scalability issue for large datasets.\n",
    "parser.add_argument('--featureless', type=str, nargs='?', default='False',\n",
    "                    help='True if one-hot encoding.')\n",
    "\n",
    "parser.add_argument('--max_gradient_norm', type=float, nargs='?', default=1.0,\n",
    "                    help='Clip gradients to this norm')\n",
    "\n",
    "# Tunable hyper-params\n",
    "\n",
    "# TODO: Implementation has not been verified, performance may not be good.\n",
    "parser.add_argument('--use_residual', type=str, nargs='?', default='False',\n",
    "                    help='Use residual')\n",
    "\n",
    "# Number of negative samples per positive pair.\n",
    "parser.add_argument('--neg_sample_size', type=int, nargs='?', default=10,\n",
    "                    help='# negative samples per positive')\n",
    "\n",
    "# Walk length for random walk sampling.\n",
    "parser.add_argument('--walk_len', type=int, nargs='?', default=20,\n",
    "                    help='Walk length for random walk sampling')\n",
    "\n",
    "# Weight for negative samples in the binary cross-entropy loss function.\n",
    "parser.add_argument('--neg_weight', type=float, nargs='?', default=1.0,\n",
    "                    help='Weightage for negative samples')\n",
    "\n",
    "parser.add_argument('--learning_rate', type=float, nargs='?', default=0.001,\n",
    "                    help='Initial learning rate for self-attention model.')\n",
    "\n",
    "parser.add_argument('--spatial_drop', type=float, nargs='?', default=0.1,\n",
    "                    help='Spatial (structural) attention Dropout (1 - keep probability).')\n",
    "\n",
    "parser.add_argument('--temporal_drop', type=float, nargs='?', default=0.5,\n",
    "                    help='Temporal attention Dropout (1 - keep probability).')\n",
    "\n",
    "parser.add_argument('--weight_decay', type=float, nargs='?', default=0.0005,\n",
    "                    help='Initial learning rate for self-attention model.')\n",
    "\n",
    "# Architecture params\n",
    "\n",
    "parser.add_argument('--structural_head_config', type=str, nargs='?', default='16,8,8',\n",
    "                    help='Encoder layer config: # attention heads in each GAT layer')\n",
    "\n",
    "parser.add_argument('--structural_layer_config', type=str, nargs='?', default='128',\n",
    "                    help='Encoder layer config: # units in each GAT layer')\n",
    "\n",
    "parser.add_argument('--temporal_head_config', type=str, nargs='?', default='16',\n",
    "                    help='Encoder layer config: # attention heads in each Temporal layer')\n",
    "\n",
    "parser.add_argument('--temporal_layer_config', type=str, nargs='?', default='128',\n",
    "                    help='Encoder layer config: # units in each Temporal layer')\n",
    "\n",
    "parser.add_argument('--position_ffn', type=str, nargs='?', default='True',\n",
    "                    help='Position wise feedforward')\n",
    "\n",
    "parser.add_argument('--window', type=int, nargs='?', default=-1,\n",
    "                    help='Window for temporal attention (default : -1 => full)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "min_time = int(args.min_time)\n",
    "max_time = int(args.max_time)\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "def get_input():\n",
    "    yes = {'yes', 'y', 'ye'}\n",
    "    no = {'no', 'n'}\n",
    "\n",
    "    while True:\n",
    "        choice = raw_input(\"Enter your choice (yes/no) (yes => continue without any changes, no => exit) : \").lower()\n",
    "        if choice in yes:\n",
    "            return True\n",
    "        elif choice in no:\n",
    "            return False\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \\n\")\n",
    "\n",
    "\n",
    "print (args)\n",
    "output_dir = \"./logs/\" + args.base_model + \"_\" + args.model\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(output_dir + '/flags_{}.json'.format(args.dataset), 'w') as outfile:\n",
    "    json.dump(vars(args), outfile)\n",
    "\n",
    "with open(output_dir + '/flags_{}.txt'.format(args.dataset), 'w') as outfile:\n",
    "    for k, v in vars(args).items():\n",
    "        outfile.write(\"{}\\t{}\\n\".format(k, v))\n",
    "\n",
    "# Dump args to flags file.\n",
    "\n",
    "train_file = \"train_old.py\" if args.base_model == \"DySAT\" else \"train_incremental.py\"\n",
    "\n",
    "# Here, t=2 => learn on graph (idx = 0) and predict the links of graph (idx = 1).\n",
    "commands = []\n",
    "for t in range(args.min_time, args.max_time + 1):\n",
    "    commands.append(' '.join(\n",
    "        [\"python\", train_file, \"--time_steps\", str(t), \"--base_model\", args.base_model, \"--model\", args.model,\n",
    "         \"--dataset\", args.dataset]))\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "print (\"Args parallel param: \", args.run_parallel)\n",
    "\n",
    "if str2bool(args.run_parallel) and args.base_model == 'DySAT':\n",
    "    print (\"Running time steps {} to {} in parallel on GPU {}\".format(args.min_time, args.max_time, args.GPU_ID))\n",
    "    processes = []\n",
    "    for cmd in commands:\n",
    "        pid = Popen(cmd, shell=True)\n",
    "        time.sleep(10)\n",
    "        processes.append(pid)\n",
    "\n",
    "    for p in processes:\n",
    "        p.wait()\n",
    "else:\n",
    "    print (\"Running time steps {} to {} sequentially on GPU {}\".format(args.min_time, args.max_time, args.GPU_ID))\n",
    "    time.sleep(1)\n",
    "    for cmd in commands:\n",
    "        print (\"Call \", cmd)\n",
    "        call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutdown is processing...\r\n"
     ]
    }
   ],
   "source": [
    "!../../root/shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
